{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e64f760",
   "metadata": {},
   "source": [
    "# IJEPA Pretrained Model Evaluation and MLP Decoder Training\n",
    "This notebook demonstrates how to:\n",
    "- Load a pretrained IJEPA checkpoint and evaluate it on a test set (average test loss).\n",
    "- Prepare and train an MLP decoder for classification using labels from `.xlsx` files.\n",
    "- Keep each major step in a separate cell for clarity and reproducibility.\n",
    "\n",
    "**Update the file paths as needed for your setup.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceeca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "torch.set_num_threads(16)\n",
    "print(f\"Number of threads: {torch.get_num_threads()}\")\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# add the path to the src directory\n",
    "import sys\n",
    "# Use a raw string to avoid unicode escape issues on Windows\n",
    "sys.path.append(r\"C:\\Users\\dash\\Documents\\Wosler\\learning_ai\\ijepa\")\n",
    "from src.helper import init_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e84cd4c",
   "metadata": {},
   "source": [
    "## Load Config, and Checkpoint\n",
    "- Loads the experiment config and pretrained checkpoint.\n",
    "- Initializes the encoder and predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3ff00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load config and checkpoint, prepare test set\n",
    "# --- Set your paths here ---\n",
    "config_path = r'..\\configs\\simUS_dataset_new_vit_tiny_16_ep100.yaml'\n",
    "checkpoint_path = r'..\\exp_logs\\vit_tiny_with_val_ep100\\vit_tiny_with_val_ep100-latest.pth.tar'\n",
    "\n",
    "# Load config\n",
    "def load_yaml_config(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "cfg = load_yaml_config(config_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Init model and predictor\n",
    "encoder, predictor = init_model(\n",
    "    device=device,\n",
    "    patch_size=cfg[\"mask\"][\"patch_size\"],\n",
    "    crop_size=cfg[\"data\"][\"crop_size\"],\n",
    "    pred_depth=cfg[\"meta\"][\"pred_depth\"],\n",
    "    pred_emb_dim=cfg[\"meta\"][\"pred_emb_dim\"],\n",
    "    model_name=cfg[\"meta\"][\"model_name\"]\n",
    ")\n",
    "encoder.eval()\n",
    "predictor.eval()\n",
    "\n",
    "# Remove 'module.' prefix if present in checkpoint keys\n",
    "def remove_module_prefix(state_dict):\n",
    "    return {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "\n",
    "# Load checkpoint\n",
    "ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "encoder.load_state_dict(remove_module_prefix(ckpt[\"encoder\"]), strict=False)\n",
    "predictor.load_state_dict(remove_module_prefix(ckpt[\"predictor\"]), strict=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a259d2a",
   "metadata": {},
   "source": [
    "# Preparing context and target patches from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafdf2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Visualize context (top half) and target (bottom half) patches on an image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# --- Set your image path here ---\n",
    "image_path = r'C:\\Users\\dash\\Documents\\Wosler\\learning_ai\\ijepa\\dataset\\dataset_DATE_2025_06_17_TIME_12_50_58\\train\\class0\\11.png'\n",
    "\n",
    "# Load image\n",
    "img = Image.open(image_path).convert('RGB')\n",
    "img_np = np.array(img)\n",
    "\n",
    "# Get image dimensions\n",
    "h, w, _ = img_np.shape\n",
    "\n",
    "# Define context (top half) and target (bottom half) regions\n",
    "context_rect = (0, 0, w, h // 2)  # (x, y, width, height)\n",
    "target_rect = (0, h // 2, w, h - h // 2)\n",
    "\n",
    "# Plot image with overlays\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.imshow(img_np)\n",
    "# Context patch (blue, semi-transparent)\n",
    "rect1 = patches.Rectangle((context_rect[0], context_rect[1]), context_rect[2], context_rect[3],\n",
    "                          linewidth=2, edgecolor='blue', facecolor='blue', alpha=0.3, label='Context (Top Half)')\n",
    "ax.add_patch(rect1)\n",
    "# Target patch (red, semi-transparent)\n",
    "rect2 = patches.Rectangle((target_rect[0], target_rect[1]), target_rect[2], target_rect[3],\n",
    "                          linewidth=2, edgecolor='red', facecolor='red', alpha=0.3, label='Target (Bottom Half)')\n",
    "ax.add_patch(rect2)\n",
    "ax.set_axis_off()\n",
    "plt.legend(handles=[rect1, rect2], loc='upper right')\n",
    "plt.title('Context (Blue) and Target (Red) Patches')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56913e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Dataset and DataLoader for training with Excel labels\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Use the crop size and normalization expected by the I-JEPA encoder\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((cfg['data']['crop_size'], cfg['data']['crop_size'])),\n",
    "    transforms.ToTensor(),\n",
    "    # Uncomment and adjust normalization if your model expects it:\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Set patch_size and crop_size for dataset usage\n",
    "patch_size = cfg['mask']['patch_size']\n",
    "crop_size = cfg['data']['crop_size']\n",
    "\n",
    "class SolidHollowDataset(Dataset):\n",
    "    def __init__(self, image_folder, excel_path, preprocess, patch_size, crop_size, device):\n",
    "        self.image_folder = image_folder\n",
    "        self.preprocess = preprocess\n",
    "        self.patch_size = patch_size\n",
    "        self.crop_size = crop_size\n",
    "        self.device = device\n",
    "\n",
    "        # Read Excel and create mapping\n",
    "        df = pd.read_excel(excel_path)\n",
    "        self.image_to_label = dict(zip(df['image'], df['material_fill']))\n",
    "        self.image_paths = [os.path.join(image_folder, fname) for fname in self.image_to_label.keys()]\n",
    "\n",
    "        # Map class names to integer labels\n",
    "        self.class_map = {'solid': 0, 'hollow': 1}\n",
    "\n",
    "        # Precompute mask indices\n",
    "        self.num_patches_h = crop_size // patch_size\n",
    "        self.num_patches_w = crop_size // patch_size\n",
    "        context_mask_np = np.zeros((self.num_patches_h, self.num_patches_w), dtype=np.float32)\n",
    "        context_mask_np[:self.num_patches_h // 2, :] = 1.0\n",
    "        self.context_indices = torch.nonzero(torch.from_numpy(context_mask_np).flatten(), as_tuple=False).flatten()\n",
    "        target_mask_np = np.zeros((self.num_patches_h, self.num_patches_w), dtype=np.float32)\n",
    "        target_mask_np[self.num_patches_h // 2:, :] = 1.0\n",
    "        self.target_indices = torch.nonzero(torch.from_numpy(target_mask_np).flatten(), as_tuple=False).flatten()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        input_tensor = self.preprocess(img).to(self.device)  # [3, crop, crop]\n",
    "        context_idx = self.context_indices.to(self.device)   # [num_context]\n",
    "        target_idx = self.target_indices.to(self.device)     # [num_target]\n",
    "        label_str = self.image_to_label[os.path.basename(img_path)]\n",
    "        label = self.class_map[label_str]\n",
    "        return input_tensor, context_idx, target_idx, label\n",
    "\n",
    "# Example usage:\n",
    "image_folder = r'C:\\Users\\dash\\Documents\\Wosler\\learning_ai\\ijepa\\dataset\\dataset_DATE_2025_06_17_TIME_15_32_38\\train\\class0'\n",
    "excel_path = r'C:\\Users\\dash\\Documents\\Wosler\\learning_ai\\ijepa\\dataset\\dataset_DATE_2025_06_17_TIME_15_32_38\\train\\class0\\class_info.xlsx'\n",
    "dataset = SolidHollowDataset(\n",
    "    image_folder=image_folder,\n",
    "    excel_path=excel_path,\n",
    "    preprocess=preprocess,\n",
    "    patch_size=patch_size,\n",
    "    crop_size=crop_size,\n",
    "    device=device\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Check a batch\n",
    "for input_tensor, context_idx, target_idx, label in dataloader:\n",
    "    print(input_tensor.shape, context_idx.shape, target_idx.shape, label)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d5452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Full training loop for MLP decoder (100 epochs)\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# Define a simple MLP decoder\n",
    "class MLPDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Re-create DataLoader with more workers for speed\n",
    "num_workers = 16  # Use all physical cores\n",
    "batch_size = 32  # Increase batch size if memory allows\n",
    "\n",
    "# Set pin_memory only if using CUDA\n",
    "pin_memory = torch.cuda.is_available()\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "mlp = None\n",
    "optimizer = None\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 100\n",
    "loss_history = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "    for input_tensor, context_idx, target_idx, label in dataloader:\n",
    "        # input_tensor: [B, 3, crop, crop], context_idx: [B, num_context], target_idx: [B, num_target]\n",
    "        with torch.no_grad():\n",
    "            z = encoder(input_tensor, context_idx.long())  # [B, N, D]\n",
    "            h = predictor(z, context_idx.long(), target_idx.long())  # [B, num_target, D]\n",
    "        pooled = h.mean(dim=1)  # [B, D]\n",
    "        if mlp is None:\n",
    "            mlp = MLPDecoder(input_dim=pooled.shape[1]).to(device)\n",
    "            optimizer = optim.Adam(mlp.parameters(), lr=1e-3)\n",
    "        logits = mlp(pooled)  # [B, 2]\n",
    "        loss = criterion(logits, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    loss_history.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Training complete in {elapsed/60:.2f} minutes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
