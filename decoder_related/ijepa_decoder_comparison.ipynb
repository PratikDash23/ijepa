{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed3dcaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14))\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0-31): 32 x Block(\n",
      "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      ")\n",
      "Mask shape: torch.Size([1, 201]), Input tensor shape: torch.Size([1, 256, 1280])\n",
      "Mask shape: torch.Size([1, 201]), Input tensor shape: torch.Size([1, 256, 384])\n",
      "Mask shape: torch.Size([1, 24]), Input tensor shape: torch.Size([1, 256, 384])\n",
      "Loaded 50889 images from c:/Users/dash/Documents/learning_ai/ijepa/datasets/ImageNet-Sketch/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"c:/Users/dash/Documents/learning_ai/ijepa\")\n",
    "from src.helper import init_model\n",
    "from src.masks.multiblock import MaskCollator\n",
    "\n",
    "class SketchFolderDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.image_paths = list(self.root_dir.rglob(\"*.png\")) + list(self.root_dir.rglob(\"*.jpg\")) + list(self.root_dir.rglob(\"*.jpeg\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "def prepare_dataloader(data_root, batch_size=32, num_workers=2):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    dataset = SketchFolderDataset(data_root, transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    return dataloader\n",
    "\n",
    "class CNNDecoder(nn.Module):\n",
    "    def __init__(self, input_dim=384, patch_size=14):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, 128 * 3 * 3)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2),  # 3x3 -> 7x7\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2),   # 7x7 -> 15x15\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 3, kernel_size=1),                        # 15x15 -> 15x15 RGB\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)  # [B, 128 * 3 * 3]\n",
    "        x = x.view(-1, 128, 3, 3)\n",
    "        x = self.decoder(x)\n",
    "        return nn.functional.interpolate(x, size=(self.patch_size, self.patch_size), mode='bilinear')\n",
    "\n",
    "class ShallowDecoder(nn.Module):\n",
    "    def __init__(self, input_dim=384, patch_size=14):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 3 * patch_size * patch_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.decoder(x)\n",
    "        return out.view(-1, 3, self.patch_size, self.patch_size)\n",
    "\n",
    "def extract_patches(img_tensor, indices, patch_size, grid_w):\n",
    "    indices = indices.flatten()  # Ensure indices is 1D\n",
    "    patches = []\n",
    "    for idx in indices:\n",
    "        idx = int(idx)  # Now idx is a Python int\n",
    "        row = idx // grid_w\n",
    "        col = idx % grid_w\n",
    "        y, x = row * patch_size, col * patch_size\n",
    "        patch = img_tensor[:, :, y:y+patch_size, x:x+patch_size]\n",
    "        patches.append(patch)\n",
    "    return torch.cat(patches, dim=0)\n",
    "\n",
    "def show_comparison(input_tensor, recon_canvas, context_masks, target_masks, patch_size, grid_size, save_path):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "    input_img = input_tensor * std + mean\n",
    "    recon_img = recon_canvas * std + mean\n",
    "    input_img = input_img.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "    recon_img = recon_img.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    context_indices = context_masks[0].squeeze(0)\n",
    "    target_indices = target_masks[0].squeeze(0)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 7))\n",
    "    for ax, img, title in zip(axs, [input_img, recon_img], [\"Original\", \"Reconstructed\"]):\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(title)\n",
    "        ax.axis(\"off\")\n",
    "        for idx in context_indices:\n",
    "            row, col = idx // grid_size, idx % grid_size\n",
    "            ax.add_patch(plt.Rectangle((col * patch_size, row * patch_size), patch_size, patch_size,\n",
    "                                       color='blue', alpha=0.3))\n",
    "        for idx in target_indices:\n",
    "            row, col = idx // grid_size, idx % grid_size\n",
    "            ax.add_patch(plt.Rectangle((col * patch_size, row * patch_size), patch_size, patch_size,\n",
    "                                       edgecolor='red', facecolor='none', linewidth=2))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "def train_decoder(model, predictor, decoder, dataloader, mask_collator, device, patch_size, grid_w, num_batches=5):\n",
    "    optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3)\n",
    "    decoder.train()\n",
    "\n",
    "    print(\"Starting efficient decoder training\")\n",
    "\n",
    "    for batch_count, batch_imgs in enumerate(dataloader):\n",
    "        if batch_count >= num_batches:\n",
    "            break\n",
    "\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "\n",
    "        try:\n",
    "            _, context_masks, target_masks = mask_collator(batch_imgs)\n",
    "        except Exception:\n",
    "            # fallback if needed\n",
    "            batch_img_list = torch.cat([img.unsqueeze(0) for img in batch_imgs], dim=0)\n",
    "            import time\n",
    "            start = time.time()\n",
    "            _, context_masks, target_masks = mask_collator(batch_img_list)\n",
    "            print(f\"Masking took {time.time() - start:.2f} seconds\")\n",
    "\n",
    "\n",
    "    #     with torch.no_grad():\n",
    "    #         z = model(batch_imgs, context_masks)\n",
    "    #         p = predictor(z, context_masks, target_masks)\n",
    "\n",
    "    #     loss_total = 0\n",
    "    #     optimizer.zero_grad()\n",
    "\n",
    "    #     for i in range(batch_imgs.size(0)):\n",
    "    #         input_img = batch_imgs[i].unsqueeze(0)  # [1, 3, 224, 224]\n",
    "    #         target_indices = target_masks[i][0].squeeze(0)\n",
    "    #         pred_tokens = p[i]\n",
    "\n",
    "    #         gt_patches = extract_patches(input_img, target_indices, patch_size, grid_w).to(device)\n",
    "    #         decoded = decoder(pred_tokens)\n",
    "\n",
    "    #         loss = nn.functional.mse_loss(decoded, gt_patches)\n",
    "    #         loss_total += loss\n",
    "\n",
    "    #     avg_loss = loss_total / batch_imgs.size(0)\n",
    "    #     avg_loss.backward()\n",
    "    #     optimizer.step()\n",
    "\n",
    "    #     print(f\"Batch {batch_count+1}/{num_batches} — Avg Loss: {avg_loss.item():.4f}\")\n",
    "\n",
    "    print(\"Decoder training completed\")\n",
    "\n",
    "def visualize_masks(input_tensor, patch_size, context_masks, target_masks, save_path):\n",
    "    # Denormalize the input tensor for visualization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "    denormalized_tensor = input_tensor * std + mean\n",
    "    denormalized_tensor = torch.clamp(denormalized_tensor, 0, 1)  # Clip to [0, 1]\n",
    "\n",
    "    # get the grid size from the patch size and input image size\n",
    "    grid_size = denormalized_tensor.shape[2] // patch_size\n",
    "\n",
    "    # prepare a figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    ax.imshow(denormalized_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy())\n",
    "    ax.set_title(\"Context and target patches\", fontsize=20)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # in the second image, fill the context patches with blue and target patches with red\n",
    "    # with alpha=0.5\n",
    "    for mask in context_masks:\n",
    "        ax.add_patch(plt.Rectangle((patch_size * (mask % grid_size), patch_size * (mask // grid_size)),\n",
    "                                     patch_size, patch_size,\n",
    "                                     edgecolor='blue', facecolor='blue', alpha=0.5))\n",
    "\n",
    "    for mask in target_masks:\n",
    "        ax.add_patch(plt.Rectangle((patch_size * (mask % grid_size), patch_size * (mask // grid_size)),\n",
    "                                     patch_size, patch_size,\n",
    "                                     edgecolor='red', facecolor='red', alpha=0.5))\n",
    "\n",
    "    # save the figure\n",
    "    plt.savefig(save_path, bbox_inches='tight', pad_inches=0.1, dpi=60.7)\n",
    "    plt.close(fig)    \n",
    "\n",
    "\n",
    "cfg_path = \"c:/Users/dash/Documents/learning_ai/ijepa/configs/in1k_vith14_ep300.yaml\"\n",
    "weights_path = \"c:/Users/dash/Documents/learning_ai/ijepa/pretrained_models/IN1K-vit.h.14-300e.pth.tar\"\n",
    "cfg = yaml.safe_load(open(cfg_path))\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model, predictor = init_model(device=device,\n",
    "                                patch_size=cfg[\"mask\"][\"patch_size\"],\n",
    "                                model_name=cfg[\"meta\"][\"model_name\"],\n",
    "                                crop_size=cfg[\"data\"][\"crop_size\"],\n",
    "                                pred_depth=cfg[\"meta\"][\"pred_depth\"],\n",
    "                                pred_emb_dim=cfg[\"meta\"][\"pred_emb_dim\"])\n",
    "model.eval()\n",
    "predictor.eval()\n",
    "\n",
    "ckpt = torch.load(weights_path, map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt[\"encoder\"], strict=False)\n",
    "predictor.load_state_dict(ckpt[\"predictor\"], strict=False)\n",
    "\n",
    "# Define the image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# create a MaskCollator instance with the desired parameters\n",
    "mask_collator = MaskCollator(\n",
    "    input_size=(224, 224),\n",
    "    patch_size=cfg[\"mask\"][\"patch_size\"],\n",
    "    enc_mask_scale=(0.85, 0.85),  # Ensure context patches cover at least 85% of the image\n",
    "    pred_mask_scale=(0.05, 0.15),  # Target patches scale\n",
    "    aspect_ratio=(0.3, 3.0),\n",
    "    nenc=1,  # Number of context masks\n",
    "    npred=1,  # Number of target masks\n",
    "    min_keep=4,  # Minimum number of patches to keep\n",
    "    allow_overlap=False  # Ensure no overlap between context and target patches\n",
    ")\n",
    "\n",
    "img = Image.open(\"c:/Users/dash/Documents/learning_ai/ijepa/images/my_image.jpg\").convert(\"RGB\")\n",
    "input_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "# Wrap input_tensor in a list to simulate a batch\n",
    "# Unpack the three values returned by MaskCollator\n",
    "_, context_masks, target_masks = mask_collator([input_tensor])\n",
    "target_indices = target_masks[0].squeeze(0) # get the target indices\n",
    "# prepare an image visualizing the context and target masks\n",
    "visualize_masks(input_tensor,\n",
    "                patch_size=cfg[\"mask\"][\"patch_size\"],\n",
    "                context_masks=context_masks[0].squeeze(0),\n",
    "                target_masks=target_masks[0].squeeze(0),\n",
    "                save_path=\"c:/Users/dash/Documents/learning_ai/ijepa/images/mask_visualization.png\")\n",
    "\n",
    "# get the IJEPA prediction \n",
    "with torch.no_grad():\n",
    "    z = model(input_tensor, context_masks)\n",
    "    p = predictor(z, context_masks, target_masks)\n",
    "\n",
    "# define patch size and grid width\n",
    "patch_size = cfg[\"mask\"][\"patch_size\"]\n",
    "grid_w = cfg[\"data\"][\"crop_size\"] // patch_size\n",
    "\n",
    "# load the ImageNet-Sketch dataset\n",
    "data_root = \"c:/Users/dash/Documents/learning_ai/ijepa/datasets/ImageNet-Sketch/\"\n",
    "dataloader = prepare_dataloader(data_root)\n",
    "print(f\"Loaded {len(dataloader.dataset)} images from {data_root}\")\n",
    "\n",
    "# decoder = ShallowDecoder(input_dim=p.shape[-1], patch_size=patch_size).to(device)\n",
    "decoder = CNNDecoder(input_dim=p.shape[-1], patch_size=patch_size).to(device)\n",
    "# gt_patches = extract_patches(input_tensor, target_indices, patch_size, grid_w).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b4203ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting efficient decoder training\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDecoder training completed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# train the decoder\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43mtrain_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_collator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_w\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrid_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3)\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# decoder.train()\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# for epoch in range(300):\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# save_path = \"c:/Users/dash/Documents/learning_ai/ijepa/images/compare_targets.png\"\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# show_comparison(input_tensor, canvas, context_masks, target_masks, patch_size, grid_w, save_path)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mtrain_decoder\u001b[39m\u001b[34m(model, predictor, decoder, dataloader, mask_collator, device, patch_size, grid_w, num_batches)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting efficient decoder training\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m i = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m images from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mSketchFolderDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m     28\u001b[39m     img_path = \u001b[38;5;28mself\u001b[39m.image_paths[idx]\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     image = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRGB\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m     31\u001b[39m         image = \u001b[38;5;28mself\u001b[39m.transform(image)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dash\\Documents\\learning_ai\\ai_venv\\Lib\\site-packages\\PIL\\Image.py:984\u001b[39m, in \u001b[36mImage.convert\u001b[39m\u001b[34m(self, mode, matrix, dither, palette, colors)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mBGR;15\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBGR;16\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBGR;24\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    982\u001b[39m     deprecate(mode, \u001b[32m12\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m984\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m has_transparency = \u001b[33m\"\u001b[39m\u001b[33mtransparency\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mP\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    988\u001b[39m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dash\\Documents\\learning_ai\\ai_venv\\Lib\\site-packages\\PIL\\ImageFile.py:300\u001b[39m, in \u001b[36mImageFile.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    297\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[32m    299\u001b[39m b = b + s\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m n, err_code = \u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[32m0\u001b[39m:\n\u001b[32m    302\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def train_decoder(model, predictor, decoder, dataloader, mask_collator, device, patch_size, grid_w, num_batches=5):\n",
    "    optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3)\n",
    "    decoder.train()\n",
    "\n",
    "    print(\"Starting efficient decoder training\")\n",
    "    i = 0\n",
    "    for i, img in enumerate(dataloader.dataset):\n",
    "        i += 1\n",
    "\n",
    "    print(f\"Loaded {i} images from {data_root}\")\n",
    "\n",
    "    # for batch_count, batch_imgs in enumerate(dataloader):\n",
    "    #     if batch_count >= num_batches:\n",
    "    #         break\n",
    "\n",
    "    #     batch_imgs = batch_imgs.to(device)\n",
    "\n",
    "    #     try:\n",
    "    #         _, context_masks, target_masks = mask_collator(batch_imgs)\n",
    "    #     except Exception:\n",
    "    #         # fallback if needed\n",
    "    #         batch_img_list = torch.cat([img.unsqueeze(0) for img in batch_imgs], dim=0)\n",
    "    #         import time\n",
    "    #         start = time.time()\n",
    "    #         _, context_masks, target_masks = mask_collator(batch_img_list)\n",
    "    #         print(f\"Masking took {time.time() - start:.2f} seconds\")\n",
    "\n",
    "\n",
    "    #     with torch.no_grad():\n",
    "    #         z = model(batch_imgs, context_masks)\n",
    "    #         p = predictor(z, context_masks, target_masks)\n",
    "\n",
    "    #     loss_total = 0\n",
    "    #     optimizer.zero_grad()\n",
    "\n",
    "    #     for i in range(batch_imgs.size(0)):\n",
    "    #         input_img = batch_imgs[i].unsqueeze(0)  # [1, 3, 224, 224]\n",
    "    #         target_indices = target_masks[i][0].squeeze(0)\n",
    "    #         pred_tokens = p[i]\n",
    "\n",
    "    #         gt_patches = extract_patches(input_img, target_indices, patch_size, grid_w).to(device)\n",
    "    #         decoded = decoder(pred_tokens)\n",
    "\n",
    "    #         loss = nn.functional.mse_loss(decoded, gt_patches)\n",
    "    #         loss_total += loss\n",
    "\n",
    "    #     avg_loss = loss_total / batch_imgs.size(0)\n",
    "    #     avg_loss.backward()\n",
    "    #     optimizer.step()\n",
    "\n",
    "    #     print(f\"Batch {batch_count+1}/{num_batches} — Avg Loss: {avg_loss.item():.4f}\")\n",
    "\n",
    "    print(\"Decoder training completed\")\n",
    "\n",
    "# train the decoder\n",
    "train_decoder(model, predictor, decoder, dataloader, mask_collator, device, patch_size=patch_size, grid_w=grid_w, num_batches=5)\n",
    "\n",
    "# optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3)\n",
    "# decoder.train()\n",
    "# for epoch in range(300):\n",
    "#     optimizer.zero_grad()\n",
    "#     decoded = decoder(p.squeeze(0))\n",
    "#     loss = nn.functional.mse_loss(decoded, gt_patches)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "# decoder.eval()\n",
    "# with torch.no_grad():\n",
    "#     recon_patches = decoder(p.squeeze(0))\n",
    "\n",
    "# canvas = input_tensor.clone()\n",
    "# count = torch.zeros_like(input_tensor)\n",
    "# for i, idx in enumerate(target_indices):\n",
    "#     row, col = idx // grid_w, idx % grid_w\n",
    "#     y, x = row * patch_size, col * patch_size\n",
    "#     canvas[:, :, y:y+patch_size, x:x+patch_size] = recon_patches[i]\n",
    "\n",
    "# save_path = \"c:/Users/dash/Documents/learning_ai/ijepa/images/compare_targets.png\"\n",
    "# show_comparison(input_tensor, canvas, context_masks, target_masks, patch_size, grid_w, save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
