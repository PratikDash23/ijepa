{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7583b03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14))\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0-31): 32 x Block(\n",
      "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      ")\n",
      "Mask shape: torch.Size([1, 201]), Input tensor shape: torch.Size([1, 256, 1280])\n",
      "Mask shape: torch.Size([1, 201]), Input tensor shape: torch.Size([1, 256, 384])\n",
      "Mask shape: torch.Size([1, 24]), Input tensor shape: torch.Size([1, 256, 384])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"c:/Users/dash/Documents/learning_ai/ijepa\")\n",
    "from src.helper import init_model\n",
    "from src.masks.multiblock import MaskCollator\n",
    "\n",
    "class SketchFolderDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.image_paths = list(self.root_dir.rglob(\"*.png\")) + list(self.root_dir.rglob(\"*.jpg\")) + list(self.root_dir.rglob(\"*.jpeg\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "def prepare_dataloader(data_root, batch_size=32, num_workers=2):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    dataset = SketchFolderDataset(data_root, transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    return dataloader\n",
    "\n",
    "class CNNDecoder(nn.Module):\n",
    "    def __init__(self, input_dim=384, patch_size=14):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, 128 * 3 * 3)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2),  # 3x3 -> 7x7\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2),   # 7x7 -> 15x15\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 3, kernel_size=1),                        # 15x15 -> 15x15 RGB\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)  # [B, 128 * 3 * 3]\n",
    "        x = x.view(-1, 128, 3, 3)\n",
    "        x = self.decoder(x)\n",
    "        return nn.functional.interpolate(x, size=(self.patch_size, self.patch_size), mode='bilinear')\n",
    "\n",
    "class ShallowDecoder(nn.Module):\n",
    "    def __init__(self, input_dim=384, patch_size=14):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 3 * patch_size * patch_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.decoder(x)\n",
    "        return out.view(-1, 3, self.patch_size, self.patch_size)\n",
    "\n",
    "def extract_patches(img_tensor, indices, patch_size, grid_w):\n",
    "    indices = indices.flatten()  # Ensure indices is 1D\n",
    "    patches = []\n",
    "    for idx in indices:\n",
    "        idx = int(idx)  # Now idx is a Python int\n",
    "        row = idx // grid_w\n",
    "        col = idx % grid_w\n",
    "        y, x = row * patch_size, col * patch_size\n",
    "        patch = img_tensor[:, :, y:y+patch_size, x:x+patch_size]\n",
    "        patches.append(patch)\n",
    "    return torch.cat(patches, dim=0)\n",
    "\n",
    "def show_comparison(input_tensor, recon_canvas, context_masks, target_masks, patch_size, grid_size, save_path):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "    input_img = input_tensor * std + mean\n",
    "    recon_img = recon_canvas * std + mean\n",
    "    input_img = input_img.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "    recon_img = recon_img.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    context_indices = context_masks[0].squeeze(0)\n",
    "    target_indices = target_masks[0].squeeze(0)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 7))\n",
    "    for ax, img, title in zip(axs, [input_img, recon_img], [\"Original\", \"Reconstructed\"]):\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(title)\n",
    "        ax.axis(\"off\")\n",
    "        for idx in context_indices:\n",
    "            row, col = idx // grid_size, idx % grid_size\n",
    "            ax.add_patch(plt.Rectangle((col * patch_size, row * patch_size), patch_size, patch_size,\n",
    "                                       color='blue', alpha=0.3))\n",
    "        for idx in target_indices:\n",
    "            row, col = idx // grid_size, idx % grid_size\n",
    "            ax.add_patch(plt.Rectangle((col * patch_size, row * patch_size), patch_size, patch_size,\n",
    "                                       edgecolor='red', facecolor='none', linewidth=2))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "def train_decoder(model, predictor, decoder, dataloader, mask_collator, device, patch_size, grid_w, num_batches=5):\n",
    "    optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3)\n",
    "    decoder.train()\n",
    "\n",
    "    print(\"Starting efficient decoder training\")\n",
    "\n",
    "    for batch_count, batch_imgs in enumerate(dataloader):\n",
    "        if batch_count >= num_batches:\n",
    "            break\n",
    "\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_img_list = [img.unsqueeze(0) for img in batch_imgs]\n",
    "        \n",
    "        # Batch masking\n",
    "        try:\n",
    "            _, context_masks, target_masks = mask_collator(tuple(batch_img_list))\n",
    "            print(f\"context_masks shape: {context_masks.shape}, target_masks shape: {target_masks.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping batch {batch_count+1} due to mask error: {e}\")\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = model(batch_imgs, context_masks)\n",
    "            p = predictor(z, context_masks, target_masks)\n",
    "\n",
    "        loss_total = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i in range(batch_imgs.size(0)):\n",
    "            input_img = batch_imgs[i].unsqueeze(0)  # [1, 3, 224, 224]\n",
    "            target_indices = target_masks[i][0].squeeze(0)\n",
    "            pred_tokens = p[i]\n",
    "\n",
    "            gt_patches = extract_patches(input_img, target_indices, patch_size, grid_w).to(device)\n",
    "            decoded = decoder(pred_tokens)\n",
    "\n",
    "            loss = nn.functional.mse_loss(decoded, gt_patches)\n",
    "            loss_total += loss\n",
    "\n",
    "        avg_loss = loss_total / batch_imgs.size(0)\n",
    "        avg_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Batch {batch_count+1}/{num_batches} ‚Äî Avg Loss: {avg_loss.item():.4f}\")\n",
    "\n",
    "    print(\"üèÅ Decoder training complete.\")\n",
    "\n",
    "def visualize_masks(input_tensor, patch_size, context_masks, target_masks, save_path):\n",
    "    # Denormalize the input tensor for visualization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "    denormalized_tensor = input_tensor * std + mean\n",
    "    denormalized_tensor = torch.clamp(denormalized_tensor, 0, 1)  # Clip to [0, 1]\n",
    "\n",
    "    # get the grid size from the patch size and input image size\n",
    "    grid_size = denormalized_tensor.shape[2] // patch_size\n",
    "\n",
    "    # prepare a figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    ax.imshow(denormalized_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy())\n",
    "    ax.set_title(\"Context and target patches\", fontsize=20)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # in the second image, fill the context patches with blue and target patches with red\n",
    "    # with alpha=0.5\n",
    "    for mask in context_masks:\n",
    "        ax.add_patch(plt.Rectangle((patch_size * (mask % grid_size), patch_size * (mask // grid_size)),\n",
    "                                     patch_size, patch_size,\n",
    "                                     edgecolor='blue', facecolor='blue', alpha=0.5))\n",
    "\n",
    "    for mask in target_masks:\n",
    "        ax.add_patch(plt.Rectangle((patch_size * (mask % grid_size), patch_size * (mask // grid_size)),\n",
    "                                     patch_size, patch_size,\n",
    "                                     edgecolor='red', facecolor='red', alpha=0.5))\n",
    "\n",
    "    # save the figure\n",
    "    plt.savefig(save_path, bbox_inches='tight', pad_inches=0.1, dpi=60.7)\n",
    "    plt.close(fig)    \n",
    "\n",
    "\n",
    "cfg_path = \"c:/Users/dash/Documents/learning_ai/ijepa/configs/in1k_vith14_ep300.yaml\"\n",
    "weights_path = \"c:/Users/dash/Documents/learning_ai/ijepa/pretrained_models/IN1K-vit.h.14-300e.pth.tar\"\n",
    "cfg = yaml.safe_load(open(cfg_path))\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model, predictor = init_model(device=device,\n",
    "                                patch_size=cfg[\"mask\"][\"patch_size\"],\n",
    "                                model_name=cfg[\"meta\"][\"model_name\"],\n",
    "                                crop_size=cfg[\"data\"][\"crop_size\"],\n",
    "                                pred_depth=cfg[\"meta\"][\"pred_depth\"],\n",
    "                                pred_emb_dim=cfg[\"meta\"][\"pred_emb_dim\"])\n",
    "model.eval()\n",
    "predictor.eval()\n",
    "\n",
    "ckpt = torch.load(weights_path, map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt[\"encoder\"], strict=False)\n",
    "predictor.load_state_dict(ckpt[\"predictor\"], strict=False)\n",
    "\n",
    "# Define the image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# create a MaskCollator instance with the desired parameters\n",
    "mask_collator = MaskCollator(\n",
    "    input_size=(224, 224),\n",
    "    patch_size=cfg[\"mask\"][\"patch_size\"],\n",
    "    enc_mask_scale=(0.85, 0.85),  # Ensure context patches cover at least 85% of the image\n",
    "    pred_mask_scale=(0.05, 0.15),  # Target patches scale\n",
    "    aspect_ratio=(0.3, 3.0),\n",
    "    nenc=1,  # Number of context masks\n",
    "    npred=1,  # Number of target masks\n",
    "    min_keep=4,  # Minimum number of patches to keep\n",
    "    allow_overlap=False  # Ensure no overlap between context and target patches\n",
    ")\n",
    "\n",
    "img = Image.open(\"c:/Users/dash/Documents/learning_ai/ijepa/images/my_image.jpg\").convert(\"RGB\")\n",
    "input_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "# Wrap input_tensor in a list to simulate a batch\n",
    "# Unpack the three values returned by MaskCollator\n",
    "_, context_masks, target_masks = mask_collator([input_tensor])\n",
    "target_indices = target_masks[0].squeeze(0) # get the target indices\n",
    "# prepare an image visualizing the context and target masks\n",
    "visualize_masks(input_tensor,\n",
    "                patch_size=cfg[\"mask\"][\"patch_size\"],\n",
    "                context_masks=context_masks[0].squeeze(0),\n",
    "                target_masks=target_masks[0].squeeze(0),\n",
    "                save_path=\"c:/Users/dash/Documents/learning_ai/ijepa/images/mask_visualization.png\")\n",
    "\n",
    "# get the IJEPA prediction \n",
    "with torch.no_grad():\n",
    "    z = model(input_tensor, context_masks)\n",
    "    p = predictor(z, context_masks, target_masks)\n",
    "\n",
    "# define patch size and grid width\n",
    "patch_size = cfg[\"mask\"][\"patch_size\"]\n",
    "grid_w = cfg[\"data\"][\"crop_size\"] // patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87531289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 24, 1280])\n"
     ]
    }
   ],
   "source": [
    "# load the ImageNet-Sketch dataset\n",
    "# data_root = \"c:/Users/dash/Documents/learning_ai/ijepa/datasets/ImageNet-Sketch/\"\n",
    "# dataloader = prepare_dataloader(data_root)\n",
    "print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2b7eb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decoder1 = ShallowDecoder(input_dim=p.shape[-1], patch_size=patch_size).to(device)\n",
    "gt_patches = extract_patches(input_tensor, target_indices, patch_size, grid_w).to(device)\n",
    "\n",
    "# # train the decoder\n",
    "# train_decoder(model, predictor, decoder, dataloader, mask_collator, device, patch_size=patch_size, grid_w=grid_w, num_batches=5)\n",
    "\n",
    "optimizer1 = torch.optim.Adam(decoder1.parameters(), lr=1e-3)\n",
    "decoder1.train()\n",
    "for epoch in range(300):\n",
    "    optimizer1.zero_grad()\n",
    "    decoded = decoder1(p.squeeze(0))\n",
    "    loss = nn.functional.mse_loss(decoded, gt_patches)\n",
    "    loss.backward()\n",
    "    optimizer1.step()\n",
    "\n",
    "decoder1.eval()\n",
    "with torch.no_grad():\n",
    "    recon_patches1 = decoder1(p.squeeze(0))\n",
    "\n",
    "canvas = input_tensor.clone()\n",
    "count = torch.zeros_like(input_tensor)\n",
    "for i, idx in enumerate(target_indices):\n",
    "    row, col = idx // grid_w, idx % grid_w\n",
    "    y, x = row * patch_size, col * patch_size\n",
    "    canvas[:, :, y:y+patch_size, x:x+patch_size] = recon_patches1[i]\n",
    "\n",
    "save_path = \"c:/Users/dash/Documents/learning_ai/ijepa/images/compare_targets_nn.png\"\n",
    "show_comparison(input_tensor, canvas, context_masks, target_masks, patch_size, grid_w, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fe98fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder2 = CNNDecoder(input_dim=p.shape[-1], patch_size=patch_size).to(device)\n",
    "gt_patches = extract_patches(input_tensor, target_indices, patch_size, grid_w).to(device)\n",
    "\n",
    "# Now let's do the same for the CNN decoder\n",
    "optimizer2 = torch.optim.Adam(decoder2.parameters(), lr=1e-3)\n",
    "decoder2.train()\n",
    "for epoch in range(300):\n",
    "    optimizer2.zero_grad()\n",
    "    decoded = decoder2(p.squeeze(0))\n",
    "    loss = nn.functional.mse_loss(decoded, gt_patches)\n",
    "    loss.backward()\n",
    "    optimizer2.step()\n",
    "decoder2.eval()\n",
    "with torch.no_grad():\n",
    "    recon_patches2 = decoder2(p.squeeze(0))\n",
    "canvas = input_tensor.clone()\n",
    "count = torch.zeros_like(input_tensor)\n",
    "for i, idx in enumerate(target_indices):\n",
    "    row, col = idx // grid_w, idx % grid_w\n",
    "    y, x = row * patch_size, col * patch_size\n",
    "    canvas[:, :, y:y+patch_size, x:x+patch_size] = recon_patches2[i]\n",
    "save_path = \"c:/Users/dash/Documents/learning_ai/ijepa/images/compare_targets_cnn.png\"\n",
    "show_comparison(input_tensor, canvas, context_masks, target_masks, patch_size, grid_w, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de07cf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------- MAE Decoder Definition ------------------\n",
    "class MAEDecoder(nn.Module):\n",
    "    def __init__(self, input_dim=1280, decoder_dim=512, patch_size=14, depth=8, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.decoder_embed = nn.Linear(input_dim, decoder_dim)\n",
    "        self.decoder_blocks = nn.Sequential(*[\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=decoder_dim, nhead=num_heads,\n",
    "                dim_feedforward=decoder_dim * 4,\n",
    "                activation=\"gelu\", batch_first=True\n",
    "            ) for _ in range(depth)\n",
    "        ])\n",
    "        self.decoder_norm = nn.LayerNorm(decoder_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_dim, patch_size * patch_size * 3)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(0)  # [1, N, D]\n",
    "        x = self.decoder_embed(x)\n",
    "        x = self.decoder_blocks(x)\n",
    "        x = self.decoder_norm(x)\n",
    "        x = self.decoder_pred(x)\n",
    "        x = x.view(-1, 3, self.patch_size, self.patch_size)\n",
    "        return x\n",
    "\n",
    "# ----------------- Load MAE Decoder Weights ------------------\n",
    "ckpt_path = \"c:/Users/dash/Documents/learning_ai/ijepa/mae/pre-trained_checkpoints/mae_pretrain_vit_base.pth\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "decoder_state = {}\n",
    "for k, v in checkpoint['model'].items():\n",
    "    if k.startswith(\"decoder_embed\") or k.startswith(\"decoder_blocks\") or k.startswith(\"decoder_norm\") or k.startswith(\"decoder_pred\"):\n",
    "        new_k = k.replace(\"decoder_\", \"\")\n",
    "        decoder_state[new_k] = v\n",
    "\n",
    "decoder3 = MAEDecoder(input_dim=p.shape[-1], decoder_dim=512, patch_size=patch_size, depth=8, num_heads=8).to(device)\n",
    "decoder3.load_state_dict(decoder_state, strict=False)\n",
    "\n",
    "# ----------------- Train Decoder on One Image ------------------\n",
    "optimizer3 = torch.optim.Adam(decoder3.parameters(), lr=1e-3)\n",
    "decoder3.train()\n",
    "for epoch in range(300):\n",
    "    optimizer3.zero_grad()\n",
    "    decoded = decoder3(p.squeeze(0))  # [N_target, 3, 14, 14]\n",
    "    loss = nn.functional.mse_loss(decoded, gt_patches)\n",
    "    loss.backward()\n",
    "    optimizer3.step()\n",
    "\n",
    "# ----------------- Reconstruct and Save Visualization ------------------\n",
    "decoder3.eval()\n",
    "with torch.no_grad():\n",
    "    recon_patches3 = decoder3(p.squeeze(0))  # [N_target, 3, 14, 14]\n",
    "\n",
    "canvas = input_tensor.clone()\n",
    "for i, idx in enumerate(target_indices):\n",
    "    row, col = idx // grid_w, idx % grid_w\n",
    "    y, x = row * patch_size, col * patch_size\n",
    "    canvas[:, :, y:y+patch_size, x:x+patch_size] = recon_patches3[i]\n",
    "\n",
    "save_path = \"c:/Users/dash/Documents/learning_ai/ijepa/images/compare_targets_mae.png\"\n",
    "show_comparison(input_tensor, canvas, context_masks, target_masks, patch_size, grid_w, save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
